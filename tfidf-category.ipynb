{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to c:\\Users\\fuzzi\\AppData\\Lo\n",
      "[nltk_data]     cal\\Programs\\Python\\Python311\\lib\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to c:\\Users\\fuzzi\\AppData\\Loca\n",
      "[nltk_data]     l\\Programs\\Python\\Python311\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to c:\\Users\\fuzzi\\AppData\\Lo\n",
      "[nltk_data]     cal\\Programs\\Python\\Python311\\lib\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "import wordcloud\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the files\n",
    "df = pd.read_csv('data/grants.csv')\n",
    "df_amt = pd.read_csv('raw data/CA.US-Social.csv', encoding='ISO-8859-1')\n",
    "df.rename(columns={'Project title': 'Project_Title'}, inplace=True)\n",
    "merged_df = pd.merge(df, df_amt, on='Project_Title', how ='left')\n",
    "merged_df = merged_df[['Project_Title', 'CIHR contribution', 'CIHR_Contribution', 'Abstract/Summary' ]]\n",
    "merged_df = merged_df.replace('nan', np.nan)\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df['CIHR_Contribution'] = merged_df['CIHR_Contribution'].replace({'\\$': '', ',': ''}, regex=True).astype(int)\n",
    "merged_df['CIHR contribution'] = merged_df['CIHR contribution'].astype(int)\n",
    "\n",
    "mismatched_indices = merged_df[merged_df['CIHR contribution'] != merged_df['CIHR_Contribution']].index\n",
    "# Drop those rows\n",
    "merged_df = merged_df.drop(mismatched_indices)  \n",
    "merged_df = merged_df[merged_df['CIHR contribution'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to c:\\Users\\fuzzi\\AppData\\Loca\n",
      "[nltk_data]     l\\Programs\\Python\\Python311\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import word_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "custom_stopwords = {'de', 'et', 'le', 'à', 'de', 'la', 'en', 'santé', 'pour', 'dans'}\n",
    "stop.update(custom_stopwords)\n",
    "punctuation = set(string.punctuation)\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    doc1 = text.lower()\n",
    "    doc2 = doc1.split()\n",
    "    doc3=[val for val in doc2 if val not in stop]\n",
    "    doc4=\" \".join([val for val in doc3]) \n",
    "    doc5=[val for val in doc4 if not val.isdigit()] #Exclude digits\n",
    "    doc6=\"\".join([val for val in doc5]) #compare to \"\".join(val for val in doc5)\n",
    "    doc7=[val for val in doc6 if val not in punctuation]\n",
    "    doc8=\"\".join([val for val in doc7])\n",
    "    doc9=doc8.split()\n",
    "    doc10= [lemma.lemmatize(val) for val in doc9]\n",
    "    doc11=\" \".join([val for val in doc10])\n",
    "    tokens = nltk.word_tokenize(doc11)\n",
    "    return tokens\n",
    "\n",
    "merged_df['clean_abstr'] = merged_df['Abstract/Summary'].apply(clean)\n",
    "merged_df['Combined_text'] = merged_df['clean_abstr'].apply(lambda x: ' '.join(x))\n",
    "clean_token = [token for tokens in merged_df['clean_abstr'] for token in tokens]\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "### COllocation\n",
    "abstracts = merged_df['clean_abstr'].tolist()\n",
    "\n",
    "##Finding bigrams\n",
    "bigram = gensim.models.Phrases(merged_df['clean_abstr'], min_count=5, threshold=10)\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "merged_df['tokenized_bigrams'] = [bigram_model[doc] for doc in merged_df['clean_abstr']]\n",
    "merged_df['Combined_bigram'] = merged_df['tokenized_bigrams'].apply(lambda x: ' '.join(x))\n",
    "merged_df['Grant_Amount'] = merged_df['CIHR contribution']\n",
    "labels = ['Very Small','Small', 'Moderate', 'Large', 'Very Large']\n",
    "bins = [merged_df['CIHR contribution'].min(), \n",
    "        merged_df['CIHR contribution'].quantile(0.2), \n",
    "        merged_df['CIHR contribution'].quantile(0.4), \n",
    "        merged_df['CIHR contribution'].quantile(0.6), \n",
    "        merged_df['CIHR contribution'].quantile(0.8), \n",
    "        merged_df['CIHR contribution'].max()]\n",
    "merged_df['funding_category'] = pd.cut(merged_df['CIHR contribution'], bins=bins, labels=labels, include_lowest=True)\n",
    "category_order = {\"Very Small\": 1, \"Small\": 2, \"Moderate\": 3, \"Large\": 4, \"Very Large\": 5}\n",
    "merged_df['category_numeric'] = merged_df['funding_category'].map(category_order)\n",
    "y = merged_df['category_numeric']\n",
    "### Splitting test train data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_train, x_test, y_train, y_test = train_test_split(merged_df['Combined_bigram'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(x_train)\n",
    "X_test_tfidf = tfidf.transform(x_test)\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "X_test_dense = X_test_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Multinomial Logistic Regression\n",
      "Best parameters: {'C': 1}\n",
      "Cross-validation accuracy: 0.4092\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.55      0.61       253\n",
      "           2       0.43      0.51      0.46       253\n",
      "           3       0.35      0.44      0.39       286\n",
      "           4       0.34      0.18      0.23       211\n",
      "           5       0.36      0.40      0.38       235\n",
      "\n",
      "    accuracy                           0.42      1238\n",
      "   macro avg       0.43      0.42      0.42      1238\n",
      "weighted avg       0.43      0.42      0.42      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[140  54  38   6  15]\n",
      " [ 24 129  54  12  34]\n",
      " [ 19  44 125  30  68]\n",
      " [ 10  35  76  37  53]\n",
      " [ 12  41  64  23  95]]\n",
      "\n",
      "2. Decision Tree\n",
      "Best parameters: {'max_depth': 10, 'min_samples_split': 2}\n",
      "Cross-validation accuracy: 0.3306\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.26      0.39       253\n",
      "           2       0.62      0.18      0.28       253\n",
      "           3       0.26      0.90      0.40       286\n",
      "           4       0.25      0.02      0.04       211\n",
      "           5       0.36      0.07      0.12       235\n",
      "\n",
      "    accuracy                           0.32      1238\n",
      "   macro avg       0.45      0.29      0.25      1238\n",
      "weighted avg       0.45      0.32      0.26      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 66  12 175   0   0]\n",
      " [ 12  46 181   4  10]\n",
      " [  6   7 258   8   7]\n",
      " [  3   6 184   5  13]\n",
      " [  2   3 210   3  17]]\n",
      "\n",
      "3. Random Forest\n",
      "Best parameters: {'max_depth': 20, 'n_estimators': 200}\n",
      "Cross-validation accuracy: 0.4052\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.51      0.58       253\n",
      "           2       0.43      0.45      0.44       253\n",
      "           3       0.31      0.60      0.41       286\n",
      "           4       0.43      0.06      0.11       211\n",
      "           5       0.36      0.31      0.33       235\n",
      "\n",
      "    accuracy                           0.40      1238\n",
      "   macro avg       0.44      0.39      0.37      1238\n",
      "weighted avg       0.44      0.40      0.38      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[128  48  69   0   8]\n",
      " [ 26 115  83   2  27]\n",
      " [ 20  31 171   9  55]\n",
      " [  4  32 124  13  38]\n",
      " [ 14  40 102   6  73]]\n",
      "\n",
      "5. Naive Bayes\n",
      "Best parameters: {'alpha': 1}\n",
      "Cross-validation accuracy: 0.3793\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.33      0.47       253\n",
      "           2       0.36      0.57      0.44       253\n",
      "           3       0.32      0.55      0.41       286\n",
      "           4       0.39      0.04      0.08       211\n",
      "           5       0.33      0.32      0.33       235\n",
      "\n",
      "    accuracy                           0.38      1238\n",
      "   macro avg       0.45      0.36      0.34      1238\n",
      "weighted avg       0.45      0.38      0.36      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 83  87  66   2  15]\n",
      " [  8 144  70   1  30]\n",
      " [  4  61 157   7  57]\n",
      " [  2  41 106   9  53]\n",
      " [  4  64  87   4  76]]\n",
      "\n",
      "6. k-Nearest Neighbors (k-NN)\n",
      "Best parameters: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Cross-validation accuracy: 0.2355\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.95      0.36       253\n",
      "           2       0.53      0.07      0.13       253\n",
      "           3       0.41      0.06      0.10       286\n",
      "           4       0.23      0.03      0.05       211\n",
      "           5       0.25      0.05      0.08       235\n",
      "\n",
      "    accuracy                           0.24      1238\n",
      "   macro avg       0.33      0.23      0.14      1238\n",
      "weighted avg       0.33      0.24      0.15      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[241   1   3   1   7]\n",
      " [223  18   7   2   3]\n",
      " [239   4  16  12  15]\n",
      " [187   4   6   6   8]\n",
      " [205   7   7   5  11]]\n",
      "\n",
      "7. Linear Discriminant Analysis\n",
      "Cross-validation accuracy: 0.3034\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.37      0.32      0.34       253\n",
      "           2       0.32      0.23      0.27       253\n",
      "           3       0.32      0.31      0.31       286\n",
      "           4       0.21      0.24      0.22       211\n",
      "           5       0.22      0.30      0.25       235\n",
      "\n",
      "    accuracy                           0.28      1238\n",
      "   macro avg       0.29      0.28      0.28      1238\n",
      "weighted avg       0.29      0.28      0.28      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[81 31 44 40 57]\n",
      " [28 58 53 43 71]\n",
      " [41 31 88 59 67]\n",
      " [36 28 38 50 59]\n",
      " [31 32 56 45 71]]\n",
      "\n",
      "Summary of Cross-Validation Scores:\n",
      "Logistic Regression: 0.4092\n",
      "Decision Tree: 0.3306\n",
      "Random Forest: 0.4052\n",
      "Naive Bayes: 0.3793\n",
      "k-NN: 0.2355\n",
      "LDA: 0.3034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "results = {}\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "knn_model = KNeighborsClassifier()\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "def train_and_evaluate(model, param_grid=None):\n",
    "    if param_grid:  # Perform grid search if param_grid is provided\n",
    "        grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        if model in (gb_model, knn_model, lda_model):\n",
    "            grid.fit(X_train_dense, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            print(f\"Best parameters: {grid.best_params_}\")\n",
    "        else:\n",
    "            grid.fit(X_train_tfidf, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            print(f\"Best parameters: {grid.best_params_}\")\n",
    "\n",
    "    else:\n",
    "        if model in (gb_model, knn_model, lda_model):\n",
    "            best_model = model.fit(X_train_dense, y_train)\n",
    "        else:\n",
    "            best_model = model.fit(X_train_tfidf, y_train)\n",
    "    # Cross-validation score\n",
    "    if model in (gb_model, knn_model, lda_model):\n",
    "        cv_score = cross_val_score(best_model, X_train_dense, y_train, cv=5, scoring='accuracy').mean()\n",
    "        print(f\"Cross-validation accuracy: {cv_score:.4f}\")\n",
    "        y_pred = best_model.predict(X_test_dense)\n",
    "    else:\n",
    "        cv_score = cross_val_score(best_model, X_train_tfidf, y_train, cv=5, scoring='accuracy').mean()\n",
    "        print(f\"Cross-validation accuracy: {cv_score:.4f}\")    \n",
    "        y_pred = best_model.predict(X_test_tfidf)\n",
    "    # Evaluate on test data\n",
    "    \n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    return best_model, cv_score\n",
    "\n",
    "# 1. Multinomial Logistic Regression\n",
    "print(\"\\n1. Multinomial Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000)\n",
    "logistic_param_grid = {'C': [0.1, 1, 10]}  # Regularization parameter\n",
    "results['Logistic Regression'] = train_and_evaluate(logistic_model, logistic_param_grid)\n",
    "\n",
    "# 2. Decision Tree\n",
    "print(\"\\n2. Decision Tree\")\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_param_grid = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "results['Decision Tree'] = train_and_evaluate(tree_model, tree_param_grid)\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3. Random Forest\")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_param_grid = {'n_estimators': [100, 200], 'max_depth': [5, 10, 20]}\n",
    "results['Random Forest'] = train_and_evaluate(rf_model, rf_param_grid)\n",
    "'''\n",
    "# 4. Gradient Boosting\n",
    "print(\"\\n4. Gradient Boosting\")\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_param_grid = {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}\n",
    "results['Gradient Boosting'] = train_and_evaluate(gb_model, gb_param_grid)\n",
    "'''\n",
    "# 5. Naive Bayes\n",
    "print(\"\\n5. Naive Bayes\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_param_grid = {'alpha': [0.1, 1, 10]}  # Smoothing parameter\n",
    "results['Naive Bayes'] = train_and_evaluate(nb_model, nb_param_grid)\n",
    "\n",
    "# 6. k-Nearest Neighbors (k-NN)\n",
    "print(\"\\n6. k-Nearest Neighbors (k-NN)\")\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_param_grid = {'n_neighbors': [3, 5, 10], 'weights': ['uniform', 'distance']}\n",
    "results['k-NN'] = train_and_evaluate(knn_model, knn_param_grid)\n",
    "\n",
    "# 7. Linear Discriminant Analysis (LDA)\n",
    "print(\"\\n7. Linear Discriminant Analysis\")\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_param_grid = None  # No hyperparameters to tune for LDA\n",
    "results['LDA'] = train_and_evaluate(lda_model, lda_param_grid)\n",
    "\n",
    "# Summary of cross-validation scores\n",
    "print(\"\\nSummary of Cross-Validation Scores:\")\n",
    "for model_name, (model, score) in results.items():\n",
    "    print(f\"{model_name}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
