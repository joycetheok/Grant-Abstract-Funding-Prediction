{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "'''%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install sklearn\n",
    "%pip install nltk\n",
    "%pip install gensim==4.3.3\n",
    "%pip install wordcloud'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "import wordcloud\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the csv and merging the files into one based on title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the files\n",
    "df = pd.read_csv('grants.csv')\n",
    "df_amt = pd.read_csv('grants_amt.csv', encoding='ISO-8859-1')\n",
    "df.rename(columns={'Project title': 'Project_Title'}, inplace=True)\n",
    "merged_df = pd.merge(df, df_amt, on='Project_Title', how ='left')\n",
    "merged_df = merged_df[['Project_Title', 'CIHR contribution', 'CIHR_Contribution', 'Abstract/Summary' ]]\n",
    "merged_df = merged_df.replace('nan', np.nan)\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df['CIHR_Contribution'] = merged_df['CIHR_Contribution'].replace({'\\$': '', ',': ''}, regex=True).astype(int)\n",
    "merged_df['CIHR contribution'] = merged_df['CIHR contribution'].astype(int)\n",
    "\n",
    "mismatched_indices = merged_df[merged_df['CIHR contribution'] != merged_df['CIHR_Contribution']].index\n",
    "# Drop those rows\n",
    "merged_df = merged_df.drop(mismatched_indices)  \n",
    "merged_df = merged_df[merged_df['CIHR contribution'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "custom_stopwords = {'de', 'et', 'le', 'à', 'de', 'la', 'en', 'santé', 'pour', 'dans'}\n",
    "stop.update(custom_stopwords)\n",
    "punctuation = set(string.punctuation)\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    doc1 = text.lower()\n",
    "    doc2 = doc1.split()\n",
    "    doc3=[val for val in doc2 if val not in stop]\n",
    "    doc4=\" \".join([val for val in doc3]) \n",
    "    doc5=[val for val in doc4 if not val.isdigit()] #Exclude digits\n",
    "    doc6=\"\".join([val for val in doc5]) #compare to \"\".join(val for val in doc5)\n",
    "    doc7=[val for val in doc6 if val not in punctuation]\n",
    "    doc8=\"\".join([val for val in doc7])\n",
    "    doc9=doc8.split()\n",
    "    doc10= [lemma.lemmatize(val) for val in doc9]\n",
    "    doc11=\" \".join([val for val in doc10])\n",
    "    tokens = nltk.word_tokenize(doc11)\n",
    "    return tokens\n",
    "\n",
    "merged_df['clean_abstr'] = merged_df['Abstract/Summary'].apply(clean)\n",
    "merged_df['Combined_text'] = merged_df['clean_abstr'].apply(lambda x: ' '.join(x))\n",
    "clean_token = [token for tokens in merged_df['clean_abstr'] for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding bigrams using gensim and creating a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "### COllocation\n",
    "abstracts = merged_df['clean_abstr'].tolist()\n",
    "\n",
    "##Finding bigrams\n",
    "bigram = gensim.models.Phrases(merged_df['clean_abstr'], min_count=5, threshold=10)\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "merged_df['tokenized_bigrams'] = [bigram_model[doc] for doc in merged_df['clean_abstr']]\n",
    "merged_df['Combined_bigram'] = merged_df['tokenized_bigrams'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "### Creating a dictionary and bag of words corpus\n",
    "dictionary = Dictionary(merged_df['tokenized_bigrams'])\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in merged_df['tokenized_bigrams']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimum number of topics using coherence score for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topic_num = range(5, 50, 5)\n",
    "coherence_score = []\n",
    "\n",
    "### Evaluating coherence score for eahc number of topics\n",
    "for num in topic_num:\n",
    "    lda_model = LdaModel(corpus = bow_corpus, num_topics=num, id2word=dictionary,\n",
    "                         random_state=42)\n",
    "    coherence_model = CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                     texts=corp, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score.append(coherence_model.get_coherence())\n",
    "\n",
    "### Plotting the coherence scores\n",
    "plt.plot(topic_num, coherence_score)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Coherence score')\n",
    "plt.title('Coherence Score vs Number of Topics')\n",
    "plt.show()\n",
    "\n",
    "### Choose the optimal number of topics\n",
    "optimal_topics = topic_num[coherence_score.index(max(coherence_score))]\n",
    "print('Optimal number of topics:', optimal_topics)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "\n",
    "### LDA modeling\n",
    "lda_model = LdaModel(bow_corpus, num_topics=optimal_topics, id2word=dictionary, random_state=42)\n",
    "topic_distributions = [dict(lda_model.get_document_topics(bow)) for bow in bow_corpus]\n",
    "topic_df = pd.DataFrame.from_records(topic_distributions).fillna(0)\n",
    "topic_df.columns = [f\"Topic_{i+1}\" for i in range(optimal_topics)]\n",
    "topic_df.index = merged_df.index\n",
    "topic_df = topic_df.sort_index()\n",
    "merged_df = merged_df.sort_index()\n",
    "topic_df['Grant_Amount'] = merged_df['CIHR contribution']\n",
    "### Categorizing the grant amount\n",
    "labels = ['Very Small','Small', 'Moderate', 'Large', 'Very Large']\n",
    "bins = [topic_df['Grant_Amount'].min(), \n",
    "        topic_df['Grant_Amount'].quantile(0.2), \n",
    "        topic_df['Grant_Amount'].quantile(0.4), \n",
    "        topic_df['Grant_Amount'].quantile(0.6), \n",
    "        topic_df['Grant_Amount'].quantile(0.8), \n",
    "        topic_df['Grant_Amount'].max()]\n",
    "\n",
    "\n",
    "topic_df['funding_category'] = pd.cut(topic_df['Grant_Amount'], bins=bins, labels=labels, include_lowest=True)\n",
    "category_order = {\"Very Small\": 1, \"Small\": 2, \"Moderate\": 3, \"Large\": 4, \"Very Large\": 5}\n",
    "topic_df['category_numeric'] = topic_df['funding_category'].map(category_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = topic_df.drop('funding_category', axis=1)  # Topic proportions as features\n",
    "y = topic_df['funding_category']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to relevant features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters and modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Multinomial Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy: 0.9998\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       1.00      1.00      1.00       211\n",
      "    Moderate       1.00      1.00      1.00       286\n",
      "       Small       1.00      1.00      1.00       253\n",
      "  Very Large       1.00      1.00      1.00       235\n",
      "  Very Small       1.00      1.00      1.00       253\n",
      "\n",
      "    accuracy                           1.00      1238\n",
      "   macro avg       1.00      1.00      1.00      1238\n",
      "weighted avg       1.00      1.00      1.00      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[211   0   0   0   0]\n",
      " [  0 286   0   0   0]\n",
      " [  0   0 253   0   0]\n",
      " [  0   0   0 235   0]\n",
      " [  0   0   0   0 253]]\n",
      "\n",
      "2. Decision Tree\n",
      "Best parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Cross-validation accuracy: 0.9996\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       1.00      1.00      1.00       211\n",
      "    Moderate       1.00      1.00      1.00       286\n",
      "       Small       1.00      1.00      1.00       253\n",
      "  Very Large       1.00      1.00      1.00       235\n",
      "  Very Small       1.00      1.00      1.00       253\n",
      "\n",
      "    accuracy                           1.00      1238\n",
      "   macro avg       1.00      1.00      1.00      1238\n",
      "weighted avg       1.00      1.00      1.00      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[211   0   0   0   0]\n",
      " [  0 286   0   0   0]\n",
      " [  0   0 253   0   0]\n",
      " [  0   0   0 235   0]\n",
      " [  0   0   0   0 253]]\n",
      "\n",
      "3. Random Forest\n",
      "Best parameters: {'max_depth': 10, 'n_estimators': 100}\n",
      "Cross-validation accuracy: 0.9998\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       1.00      1.00      1.00       211\n",
      "    Moderate       1.00      1.00      1.00       286\n",
      "       Small       1.00      1.00      1.00       253\n",
      "  Very Large       1.00      1.00      1.00       235\n",
      "  Very Small       1.00      1.00      1.00       253\n",
      "\n",
      "    accuracy                           1.00      1238\n",
      "   macro avg       1.00      1.00      1.00      1238\n",
      "weighted avg       1.00      1.00      1.00      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[211   0   0   0   0]\n",
      " [  0 286   0   0   0]\n",
      " [  0   0 253   0   0]\n",
      " [  0   0   0 235   0]\n",
      " [  0   0   0   0 253]]\n",
      "\n",
      "4. Gradient Boosting\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}\n",
      "Cross-validation accuracy: 0.9998\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       1.00      1.00      1.00       211\n",
      "    Moderate       1.00      1.00      1.00       286\n",
      "       Small       1.00      1.00      1.00       253\n",
      "  Very Large       1.00      1.00      1.00       235\n",
      "  Very Small       1.00      1.00      1.00       253\n",
      "\n",
      "    accuracy                           1.00      1238\n",
      "   macro avg       1.00      1.00      1.00      1238\n",
      "weighted avg       1.00      1.00      1.00      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[211   0   0   0   0]\n",
      " [  0 286   0   0   0]\n",
      " [  0   0 253   0   0]\n",
      " [  0   0   0 235   0]\n",
      " [  0   0   0   0 253]]\n",
      "\n",
      "5. Naive Bayes\n",
      "Best parameters: {'alpha': 10}\n",
      "Cross-validation accuracy: 0.7978\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       0.75      0.56      0.64       211\n",
      "    Moderate       0.64      1.00      0.78       286\n",
      "       Small       0.80      0.73      0.76       253\n",
      "  Very Large       1.00      0.83      0.91       235\n",
      "  Very Small       1.00      0.81      0.90       253\n",
      "\n",
      "    accuracy                           0.80      1238\n",
      "   macro avg       0.84      0.79      0.80      1238\n",
      "weighted avg       0.83      0.80      0.80      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[118  93   0   0   0]\n",
      " [  0 286   0   0   0]\n",
      " [  0  69 184   0   0]\n",
      " [ 40   0   0 195   0]\n",
      " [  0   0  47   0 206]]\n",
      "\n",
      "6. k-Nearest Neighbors (k-NN)\n",
      "Best parameters: {'n_neighbors': 10, 'weights': 'distance'}\n",
      "Cross-validation accuracy: 0.5098\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       0.38      0.32      0.35       211\n",
      "    Moderate       0.43      0.54      0.48       286\n",
      "       Small       0.42      0.51      0.46       253\n",
      "  Very Large       0.75      0.63      0.69       235\n",
      "  Very Small       0.67      0.52      0.58       253\n",
      "\n",
      "    accuracy                           0.51      1238\n",
      "   macro avg       0.53      0.50      0.51      1238\n",
      "weighted avg       0.53      0.51      0.51      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 68  83  18  36   6]\n",
      " [ 43 155  67   9  12]\n",
      " [ 11  65 128   3  46]\n",
      " [ 50  27   9 149   0]\n",
      " [  5  34  82   1 131]]\n",
      "\n",
      "7. Linear Discriminant Analysis\n",
      "Cross-validation accuracy: 1.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Large       0.21      0.20      0.20       211\n",
      "    Moderate       0.33      0.12      0.17       286\n",
      "       Small       0.24      0.07      0.11       253\n",
      "  Very Large       0.21      0.09      0.12       235\n",
      "  Very Small       0.23      0.68      0.34       253\n",
      "\n",
      "    accuracy                           0.23      1238\n",
      "   macro avg       0.24      0.23      0.19      1238\n",
      "weighted avg       0.25      0.23      0.19      1238\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 42  17  15  14 123]\n",
      " [ 52  34  16  23 161]\n",
      " [ 37  15  18  26 157]\n",
      " [ 31  21  16  21 146]\n",
      " [ 38  17   9  17 172]]\n",
      "\n",
      "Summary of Cross-Validation Scores:\n",
      "Logistic Regression: 0.9998\n",
      "Decision Tree: 0.9996\n",
      "Random Forest: 0.9998\n",
      "Gradient Boosting: 0.9998\n",
      "Naive Bayes: 0.7978\n",
      "k-NN: 0.5098\n",
      "LDA: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "results = {}\n",
    "def train_and_evaluate(model, param_grid=None):\n",
    "    if param_grid:  # Perform grid search if param_grid is provided\n",
    "        if model in (logistic_model, knn_model, lda_model):\n",
    "            grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            grid.fit(X_train_scaled, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            print(f\"Best parameters: {grid.best_params_}\")\n",
    "        else:\n",
    "            grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            print(f\"Best parameters: {grid.best_params_}\")\n",
    "    else:\n",
    "        best_model = model.fit(X_train, y_train)\n",
    "    \n",
    "    if model in (logistic_model, knn_model, lda_model):\n",
    "        cv_score = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()\n",
    "        print(f\"Cross-validation accuracy: {cv_score:.4f}\")\n",
    "    # Cross-validation score\n",
    "    else:\n",
    "        cv_score = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "        print(f\"Cross-validation accuracy: {cv_score:.4f}\")\n",
    "        \n",
    "    # Evaluate on test data\n",
    "    if model in (logistic_model, knn_model, lda_model):\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    return best_model, cv_score\n",
    "\n",
    "# 1. Multinomial Logistic Regression\n",
    "print(\"\\n1. Multinomial Logistic Regression\")\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000)\n",
    "logistic_param_grid = {'C': [0.1, 1, 10]}  # Regularization parameter\n",
    "results['Logistic Regression'] = train_and_evaluate(logistic_model, logistic_param_grid)\n",
    "\n",
    "# 2. Decision Tree\n",
    "print(\"\\n2. Decision Tree\")\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_param_grid = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "results['Decision Tree'] = train_and_evaluate(tree_model, tree_param_grid)\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3. Random Forest\")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_param_grid = {'n_estimators': [100, 200], 'max_depth': [5, 10, 20]}\n",
    "results['Random Forest'] = train_and_evaluate(rf_model, rf_param_grid)\n",
    "\n",
    "# 4. Gradient Boosting\n",
    "print(\"\\n4. Gradient Boosting\")\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_param_grid = {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}\n",
    "results['Gradient Boosting'] = train_and_evaluate(gb_model, gb_param_grid)\n",
    "\n",
    "# 5. Naive Bayes\n",
    "print(\"\\n5. Naive Bayes\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_param_grid = {'alpha': [0.1, 1, 10]}  # Smoothing parameter\n",
    "results['Naive Bayes'] = train_and_evaluate(nb_model, nb_param_grid)\n",
    "\n",
    "# 6. k-Nearest Neighbors (k-NN)\n",
    "print(\"\\n6. k-Nearest Neighbors (k-NN)\")\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_param_grid = {'n_neighbors': [3, 5, 10], 'weights': ['uniform', 'distance']}\n",
    "results['k-NN'] = train_and_evaluate(knn_model, knn_param_grid)\n",
    "\n",
    "# 7. Linear Discriminant Analysis (LDA)\n",
    "print(\"\\n7. Linear Discriminant Analysis\")\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_param_grid = None  # No hyperparameters to tune for LDA\n",
    "results['LDA'] = train_and_evaluate(lda_model, lda_param_grid)\n",
    "\n",
    "# Summary of cross-validation scores\n",
    "print(\"\\nSummary of Cross-Validation Scores:\")\n",
    "for model_name, (model, score) in results.items():\n",
    "    print(f\"{model_name}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
