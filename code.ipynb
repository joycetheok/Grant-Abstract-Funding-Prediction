{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing Stuff\n",
    "\n",
    "Import date of raw data file `CA.US-Social.csv`:  2024-11-17\n",
    "\n",
    "Source:  https://webapps.cihr-irsc.gc.ca/decisions/p/main.html?lang=en#fq={!tag=theme2}theme2%3A%22Social%20%2F%20Cultural%20%2F%20Environmental%20%2F%20Population%20Health%22&fq={!tag=country}country%3ACanada%20%20%20OR%20%20%20country%3A%22United%20States%20of%20America%22&sort=namesort%20asc&start=0&rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "## section 1.1\n",
    "import pandas as pd\n",
    "\n",
    "## section 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw data/CA.US-Social.csv\", encoding='latin1')\n",
    "grant_count = data.shape[0] # will use this in section 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean numeric columns\n",
    "numeric_cols = ['CIHR_Contribution', 'CIHR_Equipment']\n",
    "for column in numeric_cols:\n",
    "    data[column] = data[column].replace({'\\$': '', ',': ''}, regex=True)\n",
    "\n",
    "# remove 0's\n",
    "data['CIHR_Contribution'] = pd.to_numeric(data['CIHR_Contribution'], errors='coerce') \n",
    "data = data.query(\"CIHR_Contribution != 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Scraping Data\n",
    "\n",
    "So the search results from which I imported the data contains links to the information on the papers.  Within these links, the abstracts can be found, so once we can scrape that we're golden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues:\n",
    "- search results are split into 352 pages, with 20 results per page\n",
    "- the links to the papers are independent of their positions on the search results\n",
    "\n",
    "Link structure of search results:\n",
    "\n",
    "`https://webapps.cihr-irsc.gc.ca/decisions/p/main.html?lang=en#fq={!tag=theme2}theme2%3A%22Social%20%2F%20Cultural%20%2F%20Environmental%20%2F%20Population%20Health%22&fq={!tag=country}country%3ACanada%20%20%20OR%20%20%20country%3A%22United%20States%20of%20America%22&sort=namesort%20asc&start=`**0**`&rows=20` where `row` is a multiple of 20.  The maximum n should be the  max possible integer that satisfies 20n < [count of grants] \n",
    "\n",
    "\n",
    "\n",
    "Link structure of papers:\n",
    "\n",
    "`https://webapps.cihr-irsc.gc.ca/decisions/p/project_details.html?applId=`**462354**`&lang=en` where the `applID` is unique to each paper.\n",
    "\n",
    "Proposed steps:\n",
    "1. webscrape the `applID` of all papers\n",
    "2. use the `applID` to obtain the soup for each grant\n",
    "3. isolate abstract, project name, researcher(s) name, contribution count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual steps:\n",
    "1. download chromedriver here https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.69/win64/chromedriver-win64.zip and extract.  The extract location will differ based on operating system (Windows or Mac)\n",
    "2. initialize driver for selenium.  This step is required since the site is dynamically loaded\n",
    "3. load and scrape the `applID`s of all n pages of the search result\n",
    "    \n",
    "    a. save to `data/applids.csv`\n",
    "4. use the `applID`s to scrape the abstract, project name, researcher(s) names, contribution count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Scraping applIDs\n",
    "\n",
    "This shit took 27 mins to run.  Don't run it again unless necessary LUL (turn the raw blocks back into Python blocks if you need to run again)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Calculate number of pages\n",
    "if grant_count % 20 == 0:\n",
    "    page_count = grant_count // 20  # No extra page needed if divisible by 20\n",
    "else:\n",
    "    page_count = grant_count // 20 + 1  # Add one more page if not divisible by 20\n",
    "\n",
    "print(f\"Total pages to scrape: {page_count}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Set up Selenium WebDriver with the correct path\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "service = Service(\"C:/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "applids = []\n",
    "\n",
    "for page_num in range(page_count):\n",
    "    try:\n",
    "        start = str(page_num * 20)\n",
    "        link = 'https://webapps.cihr-irsc.gc.ca/decisions/p/main.html?lang=en#fq={!tag=theme2}theme2%3A%22Social%20%2F%20Cultural%20%2F%20Environmental%20%2F%20Population%20Health%22&fq={!tag=country}country%3ACanada%20%20%20OR%20%20%20country%3A%22United%20States%20of%20America%22&sort=namesort%20asc&start=' + start + '&rows=20'\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(15)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = soup.find_all('a', class_='pull-left')\n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "            match = re.search(r'applId=(\\d+)', href)\n",
    "            # lang = re.search(r'lang=(\\d+)', href)\n",
    "            if match:\n",
    "                applids.append(match.group(1))\n",
    "        # Add a delay between requests to avoid overloading the server and getting my ass banned\n",
    "        delay = random.uniform(3, 6)\n",
    "        time.sleep(delay)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on page {page_num}. Error details: {str(e)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import csv\n",
    "with open('data/applids.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['applId'])  # Write header\n",
    "    for applid in applids:\n",
    "        writer.writerow([applid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Scraping everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_info = pd.read_csv(\"data/applids.csv\")\n",
    "grants_applIDs = grants_info['applId'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
